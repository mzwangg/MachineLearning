{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lab6 决策树\n",
    "\n",
    "> 姓名：王茂增\\\n",
    "> 学号：2113972\\\n",
    "> 专业：计算机科学与技术\\\n",
    "> 代码：https://github.com/mzwangg/MachineLearning\n",
    "\n",
    "## 实验要求\n",
    "\n",
    "### 基本要求\n",
    "\n",
    "a)\t基于 Watermelon-train1数据集（只有离散属性），构造ID3决策树；\n",
    "\n",
    "b)\t基于构造的 ID3 决策树，对数据集 Watermelon-test1进行预测，输出分类精度；\n",
    "\n",
    "### 中级要求\n",
    "\n",
    "a)  对数据集Watermelon-train2，构造C4.5或者CART决策树，要求可以处理连续型属性；\n",
    "\n",
    "b)\t对测试集Watermelon-test2进行预测，输出分类精度；\n",
    "\n",
    "### 高级要求\n",
    "\n",
    "使用任意的剪枝算法对构造的决策树（基本要求和中级要求构造的树）进行剪枝，观察测试集合的分类精度是否有提升，给出分析过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本要求: ID3决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了在高级要求时实现剪枝，我将train.csv数据集通过**分层采样**进一步划分为了**训练集**和**验证集**，具体见代码及注释。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1986,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 对train.csv进行分层采样\n",
    "def load_train_data(file_path, seed, validation_ratio=0.3):\n",
    "    # 设置随机数种子， 以保证可复现性\n",
    "    np.random.seed(seed)\n",
    "    # 读取数据，获取列标签\n",
    "    data = np.array(pd.read_csv(file_path, encoding=\"GBK\"))[:,1:]\n",
    "    labels = data[:, -1]\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    # 初始化训练集和验证集\n",
    "    train_set = []\n",
    "    validation_set = []\n",
    "\n",
    "    # 对每个唯一标签进行分层采样\n",
    "    for label in unique_labels:\n",
    "        # 获取属于当前标签的所有样本的索引，随机打乱，计算划分点\n",
    "        indices = np.where(labels == label)[0]\n",
    "        np.random.shuffle(indices)\n",
    "        split_point = int(len(indices) * (1 - validation_ratio))\n",
    "\n",
    "        # 划分数据集\n",
    "        train_indices = indices[:split_point]\n",
    "        validation_indices = indices[split_point:]\n",
    "\n",
    "        # 将数据添加到训练集和验证集\n",
    "        train_set.append(data[train_indices])\n",
    "        validation_set.append(data[validation_indices])\n",
    "\n",
    "    # 将分层采样后的数据集合并为最终的训练集和验证集\n",
    "    train_set = np.concatenate(train_set, axis=0)\n",
    "    validation_set = np.concatenate(validation_set, axis=0)\n",
    "\n",
    "    return train_set, validation_set\n",
    "\n",
    "def load_test_data(file_path):\n",
    "    data = np.array(pd.read_csv(file_path, encoding=\"GBK\"))[:,1:]\n",
    "    return data\n",
    "\n",
    "def load_feature(file_path):\n",
    "    data = np.genfromtxt(file_path, delimiter=',', dtype=str, max_rows=1, encoding='GBK')\n",
    "    return data[1:]\n",
    "\n",
    "# 加载数据\n",
    "train_data1, valid_data1 = load_train_data(\"Watermelon-train1.csv\", 10)\n",
    "test_data1 = load_test_data(\"Watermelon-test1.csv\")\n",
    "feature1 = load_feature(\"Watermelon-train1.csv\")\n",
    "\n",
    "train_data2, valid_data2 = load_train_data(\"Watermelon-train2.csv\", 9)\n",
    "test_data2 = load_test_data(\"Watermelon-test2.csv\")\n",
    "feature2 = load_feature(\"Watermelon-train2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 信息增益：\n",
    "\n",
    "1. **信息熵（Entropy）公式**：\n",
    "\n",
    "    $ \\text{Entropy}(D) = - \\sum_{i=1}^{k} p_i \\cdot \\log_2(p_i) $\n",
    "\n",
    "    其中，$k$ 是类别数目，$p_i$ 是数据集中属于类别 $i$ 的样本占比。\n",
    "\n",
    "2. **信息增益公式**:\n",
    "\n",
    "    $ \\text{Gain}(D, A) = \\text{Entropy}(D) - \\sum_{v \\in \\text{Values}(A)} \\frac{|D_v|}{|D|} \\cdot \\text{Entropy}(D_v) $\n",
    "\n",
    "    其中，$\\text{Gain}(D, A)$ 是信息增益，$\\text{Entropy}(D)$ 是数据集 $D$ 的信息熵，$\\text{Values}(A)$ 是特征 $A$ 的取值集合，$D_v$ 是特征 $A$ 取值为 $v$ 时的子集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1987,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算信息熵\n",
    "def entropy(labels):\n",
    "    _, counts = np.unique(labels, return_counts=True)\n",
    "    probabilities = counts / len(labels)\n",
    "    return -np.sum(probabilities * np.log2(probabilities))\n",
    "\n",
    "# 计算信息增益\n",
    "def information_gain(data, feature_index):\n",
    "    features = data[:, feature_index]\n",
    "    unique_values, counts = np.unique(features, return_counts=True)\n",
    "    w_entropys = np.sum([(counts[i] / len(features)) * \n",
    "                         entropy(data[data[:, feature_index] \n",
    "                                      == unique_values[i]][:, -1]) \n",
    "                         for i in range(len(unique_values))])\n",
    "    return entropy(data[:, -1]) - w_entropys\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3决策树\n",
    "\n",
    "**ID3 (Iterative Dichotomiser 3)** 是一种用于构建决策树的经典算法，最早由Ross Quinlan于1986年提出。它是基于信息熵的一种自顶向下的递归分割算法，旨在通过选择最佳的特征进行节点分割，使得每个分割后的子集尽可能纯净。\n",
    "\n",
    "### ID3算法步骤：\n",
    "\n",
    "1. **选择最佳特征：** 通过计算每个特征的信息增益（信息熵的减少量）来选择最佳的特征作为当前节点的分割特征。信息增益表示使用该特征进行分割后数据集的不确定性减少程度。\n",
    "\n",
    "2. **为节点分配标签：** 如果当前节点的数据集所有样本都属于同一类别，则将该节点标记为叶子节点，并使用该类别标签。\n",
    "\n",
    "3. **递归：** 对于每个特征值，将数据集划分为子集，并对每个子集应用上述步骤，递归构建决策树。\n",
    "\n",
    "在递归构建决策树节点的过程中，将此时的训练集标签也加入节点，以便进行剪枝。\n",
    "\n",
    "ID3算法的主要优点是简单易懂，生成的决策树具有可解释性。然而，它对于处理连续特征和处理缺失值的能力相对较弱，这些问题后来由C4.5和CART等算法得到改进。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1988,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id3_decision_tree(data, feature_indices):\n",
    "    # 如果没有特征可用，或已经全部划分完成则返回数据集中实例数最多的类别\n",
    "    unique_labels, counts = np.unique(data[:, -1], return_counts=True)\n",
    "    if len(unique_labels) == 1 or len(feature_indices) == 0:\n",
    "        return unique_labels[np.argmax(counts)]\n",
    "    \n",
    "    # 选择最佳划分特征，并更新feature_indices\n",
    "    best_feature_index = np.argmax([information_gain(data, i) for i in feature_indices])\n",
    "    feature_indices = np.delete(feature_indices, best_feature_index)\n",
    "    \n",
    "    # 根据选取的特征在在子树上递归继续构建子树\n",
    "    # 将此时的训练集标签也加入节点，以便进行剪枝\n",
    "    tree = (best_feature_index, [], data[:, -1])\n",
    "    unique_values = np.unique(data[:, best_feature_index])\n",
    "    for value in unique_values:\n",
    "        subset = data[data[:, best_feature_index] == value]\n",
    "        subtree = id3_decision_tree(subset, feature_indices)\n",
    "        tree[1].append((value, subtree))\n",
    "    \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测及打印"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1989,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 递归进行预测\n",
    "def id3_predict(tree, sample):\n",
    "    # 当预测到类别时，直接返回预测的类别\n",
    "    if not isinstance(tree, tuple):\n",
    "        return tree\n",
    "    \n",
    "    # 根据建立的决策树递归预测\n",
    "    feature, subtrees, _ = tree\n",
    "    for subtree in subtrees:\n",
    "        if subtree[0] == sample[feature]:\n",
    "            return id3_predict(subtree[1], sample)\n",
    "\n",
    "# 打印决策树\n",
    "def print_id3_tree(node, features, indent=\"\"):\n",
    "    if isinstance(node, tuple):\n",
    "        feature_index, branches, _ = node\n",
    "        feature_name = features[feature_index]\n",
    "        print(f\"{indent}{feature_name}\")\n",
    "        for value, subtree in branches:\n",
    "            print(f\"{indent}  |---{feature_name} == {value}\")\n",
    "            print_id3_tree(subtree, features, indent + \"  |   \")\n",
    "    else:\n",
    "        print(f\"{indent}  └──类别: {node}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建树及预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1990,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_true: ['是' '是' '是' '是' '是' '否' '否' '否' '否' '否']\n",
      "test_pred: ['是' '是' '是' '否' '是' '是' '是' '否' '否' '否']\n",
      "ID3决策树分类精度: 0.7\n"
     ]
    }
   ],
   "source": [
    "# 构建ID3决策树\n",
    "feature_indices1 = np.arange(train_data1.shape[1] - 1)\n",
    "id3_tree = id3_decision_tree(train_data1, feature_indices1)\n",
    "\n",
    "# 预测测试集并计算分类精度\n",
    "test_true1 = test_data1[:, -1]\n",
    "id3_predict(id3_tree, test_data1[:, :-1][0])\n",
    "test_pred1 = np.array([id3_predict(id3_tree, sample) for sample in test_data1[:, :-1]])\n",
    "acc1 = np.sum(test_pred1 == test_true1) / len(test_true1)\n",
    "\n",
    "# 输出预测精度\n",
    "print(\"test_true:\", test_true1)\n",
    "print(\"test_pred:\", test_pred1)\n",
    "print(\"ID3决策树分类精度:\", acc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中级要求：CART决策树\n",
    "\n",
    "**CART决策树（Classification and Regression Trees）** 是由Leo Breiman于1984年提出的一种用于分类和回归的树形模型。CART决策树的主要特点是可以处理离散和连续型的特征，并且可以应用于分类和回归任务。\n",
    "\n",
    "### CART决策树的优点和缺点：\n",
    "\n",
    "**优点：**\n",
    "- 可以处理混合类型的特征（离散和连续）。\n",
    "- 生成的树具有很好的解释性。\n",
    "- 对于较小的数据集和较小的树，具有较高的计算效率。\n",
    "\n",
    "**缺点：**\n",
    "- 对于高维稀疏数据或大规模数据，可能性能不如其他模型。\n",
    "- 生成的树可能过拟合，需要剪枝等策略进行调整。\n",
    "\n",
    "CART决策树在许多实际应用中都表现良好，特别适用于二分类和回归问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基尼系数\n",
    "\n",
    "$ \\text{Gini}(D) = 1 - \\sum_{i=1}^{k} p_i^2 $\n",
    "\n",
    "其中，$k$ 是类别数目，$p_i$ 是数据集 $D$ 中属于类别 $i$ 的样本占比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1991,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据相应公式计算基尼系数\n",
    "def calculate_gini(labels):\n",
    "    _, counts = np.unique(labels, return_counts=True)\n",
    "    probabilities = counts / len(labels)\n",
    "    return 1 - np.sum(probabilities**2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CART决策树的构建过程：\n",
    "\n",
    "1. **特征选择：** CART使用**基尼系数**作为特征选择的准则。对于一个数据集，对于每个特征的每个可能的切分点，计算切分后的子集的基尼系数。选择具有最小基尼不纯度的特征和切分点。\n",
    "\n",
    "2. **递归分裂：** 将数据集根据选择的特征和切分点进行分裂，得到子集。对每个子集重复上述过程，直到满足停止条件（如树的深度达到预定值、节点中的样本数小于某个阈值等）。\n",
    "\n",
    "3. **生成叶子节点：** 当满足停止条件时，为叶子节点分配一个输出值。对于分类任务，通常选择叶子节点中样本最多的类别作为输出值；对于回归任务，通常选择叶子节点中样本的平均值作为输出值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1992,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据data在feature_index属性上按照threshold划分左右子树\n",
    "def split_dataset(data, feature_index, threshold):\n",
    "    if isinstance(threshold, str): # 处理字符串\n",
    "        left_mask = data[:, feature_index] == threshold\n",
    "        right_mask = ~left_mask\n",
    "    else: # 处理连续值\n",
    "        left_mask = data[:, feature_index] <= threshold\n",
    "        right_mask = ~left_mask\n",
    "    return data[left_mask], data[right_mask] # 返回左子树和右子树\n",
    "\n",
    "# 寻找最优划分\n",
    "def find_best_split(data):\n",
    "    best_gini = float('inf')\n",
    "    best_split = None\n",
    "    \n",
    "    for i in range(data.shape[1] - 1):# 遍历每个属性\n",
    "        unique_values = np.unique(data[:, i])\n",
    "        for value in unique_values:# 遍历属性的每个值\n",
    "            left_data, right_data = split_dataset(data, i, value) # 进行划分\n",
    "\n",
    "            # 计算左右子树的基尼系数，并进一步计算整体的基尼系数\n",
    "            gini_left = calculate_gini(left_data[:, -1])\n",
    "            gini_right = calculate_gini(right_data[:, -1])\n",
    "            gini = (len(left_data) * gini_left + len(right_data) * gini_right) / len(data)\n",
    "\n",
    "            # 选择具有最小基尼系数的划分\n",
    "            if gini < best_gini:\n",
    "                best_gini = gini\n",
    "                best_split = (i, value)\n",
    "\n",
    "    return best_split\n",
    "\n",
    "# 构建CART决策树\n",
    "def cart_decision_tree(data, max_depth=float('inf')):\n",
    "    # 当达到最大深度或者已经全部划分完成，返回数量最多的类别作为预测类别\n",
    "    unique_labels, counts = np.unique(data[:, -1], return_counts=True)\n",
    "    if max_depth == 0 or len(unique_labels) == 1:\n",
    "        return unique_labels[np.argmax(counts)]\n",
    "    \n",
    "    # 寻找的当前数据的最优划分，并得到最优划分的左右子树\n",
    "    best_split = find_best_split(data)\n",
    "    feature_index, threshold = best_split\n",
    "    left_data, right_data = split_dataset(data, feature_index, threshold)\n",
    "\n",
    "    # 对左右子树进一步进行划分\n",
    "    left_subtree = cart_decision_tree(left_data, max_depth - 1)\n",
    "    right_subtree = cart_decision_tree(right_data, max_depth - 1)\n",
    "    return [feature_index, threshold, left_subtree, right_subtree, data[:, -1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测及打印"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1993,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cart_predict(tree, sample):\n",
    "    # 如果到达了叶子结点，则tree只是一个类别。而不是tuple构成的树\n",
    "    if not isinstance(tree, list):\n",
    "        return tree\n",
    "\n",
    "    feature_index, threshold, left_subtree, right_subtree, _ = tree\n",
    "    # 根据划分属性的类别计算到达左右子树的数据序号\n",
    "    if isinstance(threshold, str):  # 如果当前划分属性是字符串\n",
    "        condition = sample[feature_index] == threshold\n",
    "    else:  # 如果当前划分属性是连续值\n",
    "        condition = sample[feature_index] <= threshold\n",
    "\n",
    "    # 选择左右子树中的一个进行递归预测\n",
    "    if condition:\n",
    "        return cart_predict(left_subtree, sample)\n",
    "    else:\n",
    "        return cart_predict(right_subtree, sample)\n",
    "\n",
    "def print_cart_tree(node, features, indent=\"\"):\n",
    "    if len(node) == 1:\n",
    "        # 叶子节点，直接打印类别\n",
    "        print(f\"{indent}Class: {node[0]}\")\n",
    "        return\n",
    "    \n",
    "    feature_index, threshold, left_subtree, right_subtree, _ = node\n",
    "    feature_name = features[feature_index]\n",
    "\n",
    "    # 打印当前节点判断条件\n",
    "    print(f\"{indent}{feature_name} <= {threshold}\")\n",
    "    # 递归打印左子树\n",
    "    print_cart_tree(left_subtree, features, indent + \"  |   \")\n",
    "    # 打印当前节点的判断条件\n",
    "    print(f\"{indent}  └── {feature_name} > {threshold}\")\n",
    "    # 递归打印右子树\n",
    "    print_cart_tree(right_subtree, features, indent + \"      \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建树及预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1994,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_true: ['是' '是' '是' '否' '否']\n",
      "test_pred: ['是' '否' '是' '是' '否']\n",
      "ID3决策树分类精度: 0.6\n"
     ]
    }
   ],
   "source": [
    "# 使用CART算法建立决策树\n",
    "cart_tree = cart_decision_tree(train_data2)\n",
    "\n",
    "# 计算预测准确率\n",
    "test_true2 = test_data2[:, -1]\n",
    "test_pred2 = np.array([cart_predict(cart_tree, sample) for sample in test_data2[:, :-1]])\n",
    "acc2 = np.sum(test_pred2 == test_true2) / len(test_true2)\n",
    "\n",
    "# 输出预测精度\n",
    "print(\"test_true:\", test_true2)\n",
    "print(\"test_pred:\", test_pred2)\n",
    "print(\"ID3决策树分类精度:\", acc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高级要求：剪枝\n",
    "\n",
    "后剪枝（post-pruning）是决策树构建过程的一个阶段，它发生在决策树已经建立完成后。后剪枝的目标是通过剪枝一些分支（子树）来提高决策树的泛化性能。以下是决策树后剪枝的一般步骤：\n",
    "\n",
    "1. **划分数据集：** 将原始数据集划分为训练集、验证集、测试集。训练集用于构建决策树，而验证集用于评估剪枝后的性能。\n",
    "\n",
    "2. **构建完整的决策树：** 首先，通过训练数据构建完整的决策树。这个决策树可能在训练数据上表现得很好，但在新数据上可能存在过拟合的问题。\n",
    "\n",
    "3. **后序遍历决策树：** 从决策树的底部（叶子节点）开始，进行后序遍历。在遍历过程中，每个节点分别作为根节点进行以下操作。\n",
    "\n",
    "4. **剪枝尝试：** 用当前节点的多数类别替换子树，并在验证集上评估性能。如果剪枝后的性能不下降（甚至有所提升），则执行剪枝操作，将子树替换为当前节点的多数类别。否则，保留原来的子树。\n",
    "\n",
    "5. **递归操作：** 递归地对每个父节点进行相同的操作，直到根节点。\n",
    "\n",
    "这样，通过在验证集上评估剪枝后的性能，决策树的复杂部分可以被替换为更简单的结构，从而提高了决策树的泛化性能。在实际应用中，我们可以调整验证集的划分比例、剪枝时机等参数来优化后剪枝的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3决策树剪枝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1995,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估id3决策树在数据集data上的精确度\n",
    "def cal_id3_acc(data):\n",
    "    true = data[:, -1]\n",
    "    pred = np.array([id3_predict(id3_tree, sample) for sample in data[:, :-1]])\n",
    "    acc = np.sum(pred == true) / len(true)\n",
    "    return acc if not np.isnan(acc) else 0\n",
    "\n",
    "# id3决策树后剪枝\n",
    "def id3_post_pruning(parent, index, tree):\n",
    "    if not isinstance(tree, tuple):\n",
    "        return\n",
    "\n",
    "    # 遍历子树，递归进行后剪枝\n",
    "    subtrees = tree[1]\n",
    "    index = tree[0]\n",
    "    for i in range(len(subtrees)):\n",
    "        subtree = subtrees[i]\n",
    "        id3_post_pruning(subtrees, i, subtree[1])\n",
    "    \n",
    "    if parent is None:\n",
    "        return\n",
    "        \n",
    "    # 计算当前节点的正确率\n",
    "    original_acc = cal_id3_acc(valid_data1)\n",
    "        \n",
    "    # 将待剪枝节点替换为训练集在该子树中最多的那个类型\n",
    "    old_tree = parent[index]\n",
    "    labels = tree[2]\n",
    "    unique_elements, counts = np.unique(labels, return_counts=True)\n",
    "    index_of_max_count = np.argmax(counts)\n",
    "    parent[index] = (old_tree[0], unique_elements[index_of_max_count])\n",
    "\n",
    "    # 计算剪枝之后在验证集的正确率\n",
    "    after_acc = cal_id3_acc(valid_data1)\n",
    "\n",
    "    # 如果合并后在验证集中正确率没有增加，则恢复子树\n",
    "    if after_acc <= original_acc:\n",
    "        parent[index] = old_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1996,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "剪枝前：\n",
      "\n",
      "纹理\n",
      "  |---纹理 == 模糊\n",
      "  |     └──类别: 否\n",
      "  |---纹理 == 清晰\n",
      "  |     └──类别: 是\n",
      "  |---纹理 == 稍糊\n",
      "  |   敲声\n",
      "  |     |---敲声 == 沉闷\n",
      "  |     |     └──类别: 否\n",
      "  |     |---敲声 == 浊响\n",
      "  |     |     └──类别: 是\n",
      "\n",
      "正确率: 0.7\n"
     ]
    }
   ],
   "source": [
    "print(\"剪枝前：\\n\")\n",
    "print_id3_tree(id3_tree, feature1)\n",
    "print(\"\\n正确率:\", cal_id3_acc(test_data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1997,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "剪枝后：\n",
      "\n",
      "纹理\n",
      "  |---纹理 == 模糊\n",
      "  |     └──类别: 否\n",
      "  |---纹理 == 清晰\n",
      "  |     └──类别: 是\n",
      "  |---纹理 == 稍糊\n",
      "  |     └──类别: 否\n",
      "\n",
      "正确率: 0.8\n"
     ]
    }
   ],
   "source": [
    "id3_post_pruning(None, -1, id3_tree)\n",
    "print(\"剪枝后：\\n\")\n",
    "print_id3_tree(id3_tree, feature1)\n",
    "print(\"\\n正确率:\", cal_id3_acc(test_data1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有上述结果可知，经过剪枝之后，当纹理等于稍糊时，决策树由原来的进一步根据敲声判断变为了直接返回否，这不仅减少了决策树的规模，也将决策树的准确率由0.7提高到了0.8。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CART决策树剪枝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1998,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估cart决策树在数据集data上的精确度\n",
    "def cal_cart_acc(data):\n",
    "    true = data[:, -1]\n",
    "    pred = np.array([cart_predict(cart_tree, sample) for sample in data[:, :-1]])\n",
    "    acc = np.sum(pred == true) / len(true)\n",
    "    return acc if not np.isnan(acc) else 0\n",
    "\n",
    "# cart决策树后剪枝\n",
    "def cart_post_pruning(parent, index, tree):\n",
    "    if not isinstance(tree, list):\n",
    "        return\n",
    "\n",
    "    # 对左右子树递归进行后剪枝\n",
    "    cart_post_pruning(tree, 2, tree[2])\n",
    "    cart_post_pruning(tree, 3, tree[3])\n",
    "    \n",
    "    if parent is None:\n",
    "        return\n",
    "        \n",
    "    # 计算当前节点的正确率\n",
    "    original_acc = cal_cart_acc(valid_data2)\n",
    "\n",
    "    # 将待剪枝节点替换为训练集在该子树中最多的那个类型\n",
    "    old_tree = parent[index]\n",
    "    labels = tree[4]\n",
    "    unique_elements, counts = np.unique(labels, return_counts=True)\n",
    "    index_of_max_count = np.argmax(counts)\n",
    "    parent[index] = unique_elements[index_of_max_count]\n",
    "\n",
    "     # 计算剪枝之后在验证集的正确率\n",
    "    after_acc = cal_cart_acc(valid_data2)\n",
    "\n",
    "    # 如果合并后在验证集中正确率没有增加，则恢复子树\n",
    "    if after_acc <= original_acc:\n",
    "        parent[index] = old_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1999,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "剪枝前：\n",
      "\n",
      "纹理 <= 清晰\n",
      "  |   根蒂 <= 硬挺\n",
      "  |     |   Class: 否\n",
      "  |     └── 根蒂 > 硬挺\n",
      "  |         Class: 是\n",
      "  └── 纹理 > 清晰\n",
      "      色泽 <= 乌黑\n",
      "        |   Class: 是\n",
      "        └── 色泽 > 乌黑\n",
      "            Class: 否\n",
      "\n",
      "正确率: 0.6\n"
     ]
    }
   ],
   "source": [
    "print(\"剪枝前：\\n\")\n",
    "print_cart_tree(cart_tree, feature2)\n",
    "print(\"\\n正确率:\", cal_cart_acc(test_data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2000,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "剪枝后：\n",
      "\n",
      "纹理 <= 清晰\n",
      "  |   根蒂 <= 硬挺\n",
      "  |     |   Class: 否\n",
      "  |     └── 根蒂 > 硬挺\n",
      "  |         Class: 是\n",
      "  └── 纹理 > 清晰\n",
      "      Class: 否\n",
      "\n",
      "正确率: 0.8\n"
     ]
    }
   ],
   "source": [
    "cart_post_pruning(None, -1, cart_tree)\n",
    "print(\"剪枝后：\\n\")\n",
    "print_cart_tree(cart_tree, feature2)\n",
    "print(\"\\n正确率:\", cal_cart_acc(test_data2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有上述结果可知，经过剪枝之后，当纹理不等于清晰时，决策树由原来的进一步根据色泽判断变为了直接返回否，这不仅减少了决策树的规模，也将决策树的准确率由0.6提高到了0.8。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
